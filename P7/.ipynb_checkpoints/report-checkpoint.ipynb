{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Choice\n",
    "\n",
    "##### 1. Number of cookies ( Invariant )\n",
    "Cookies are information kept by browsers for every site. Cookies as assigned when a user visit a site (URL) for the first time and persist within each browser until it expires or be cleared. So the number of unique cookies is mostly driven by the number of users and the number of different browsers people use to visit the site. Our experiment of showing the question should have no effect on the number of users or their browsing behaviour. Hence, we expect the number of cookies to be invariant in our experiment.\n",
    "\n",
    "##### 2. Number of user-ids\n",
    "This experiment is likely to results less user to enroll in the free trail. So it can't be used as an invariant metrics for validation.\n",
    "\n",
    "It is also a poor choice for evaluation metric. To illustrate this, suppose in our experiment, 100 unique cookies are assigned to the control group and 120 to the experiment. Then, even if there are significantly more unique-id enrolled in our experiment group than the control, we can't conclude that the experiment treatment is effective. The observed difference in `user-ids` could simply because there were more traffic diverged to the experiment group.\n",
    "\n",
    "In order to draw meaningful conclusions, the number of user-ids should be \"normalized\" by divided with the unique number of clicks. And this is in fact the gross conversion discussed below.\n",
    "\n",
    "##### 3. Number of clicks ( Invariant )\n",
    "The question is shown after users have clicked the \"Start free trail\" button. Hence, we don't expect any difference in the number of clicks between the two groups. Therefore, it can be used as an invariant metrics for validation.\n",
    "\n",
    "##### 4. Click-through-probability ( Invariant )\n",
    "The `Click-through-probability` is the ratio of `number of clicks` and `number of cookies`. As both `number of cookies` and `number of clicks` are expected to be invariant, we will expect `click-through-probability` to be invariant too.\n",
    "\n",
    "##### 5. Gross conversion (Evaluation)\n",
    "If the experiment is effective, we expect to see the `gross conversion` to decrease. That is we expect fewer proportion of student will complete the enrollment process. To launch the experiment, we requires the gross conversion to be both statistically and practically significantly less in the experiment group.\n",
    "\n",
    "##### 6. Retention (Evaluation*)\n",
    "If the experiment is effective, we would expect the experiment group to have a higher retention as the experiment. In order for us to launch the experiment, we require the `retention` in our experiment group to be both statistically and practically greater than the one in our control group.\n",
    "(As it turns out, this metrics required too long to complete for the given alpha and beta, and hence not used in the final evaluation )\n",
    "\n",
    "##### 7. Net conversion (Evaluation)\n",
    "`Net conversion` is the ratio of the `number of user-ids remind enrolled pass the trail` over the `number of clicks`. Because our hypothesis states that the experiment would not \"significantly reduce the number of students to continue past the free trial\", and as discussed earlier that we expect the number of clicks to be invariant, it follows that we expect the `net conversion` to doesn't change significantly. Hence, in order for us to launch the experiment, we require the difference of `net conversion` between the two groups to be small, both statistically and practically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Standard Deviation\n",
    "\n",
    "| Metrics | Standard Deviation | Comparable to empirical ? |\n",
    "| --- | --- |\n",
    "| Gross conversion | 0.0202 | Yes |\n",
    "| Retention | 0.0549 | No |\n",
    "| Net conversion | 0.0156 | Yes |\n",
    "\n",
    "The analytical estimate tends to be comparable to the empirically estimates when the unit of diversion and\n",
    "the unit of analysis are the same.\n",
    "\n",
    "Hence, I expect the estimate for gross conversion and net Conversion to be comparable to empirical estimate. Similarly, the empirical and analytical estimate for retention are not likely to be comparable, because the unit of analysis ( number of unique user id) is different to the unit of diversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing\n",
    "#### Number of Samples vs. Power\n",
    "\n",
    "> Indicate whether you will use the Bonferroni correction during your analysis phase, and give the number of pageviews you will need to power you experiment appropriately. (These should be the answers from the \"Calculating Number of Pageviews\" quiz.)\n",
    "\n",
    "No. I will not use Bonferroni correction duration my analysis. Because the evaluation metrics is highly correlated, using Bonferroni correction, in this case, seems to be too conservative.\n",
    "\n",
    "Using alpha = 0.05 and beta = 0.2, a total of 685,275 page views is required to conduct this experiment.\n",
    "\n",
    "#### Duration vs. Exposure\n",
    "> Indicate what fraction of traffic you would divert to this experiment and, given this, how many days you would need to run the experiment. (These should be the answers from the \"Choosing Duration and Exposure\" quiz.)\n",
    "\n",
    "> Give your reasoning for the fraction you chose to divert. How risky do you think this experiment would be for Udacity?\n",
    "\n",
    "**Risks**\n",
    "The experiment is not a risky one. It does not expose the users to risk that exceeds the \"minimal risk\" level. No sensitive information is involved in the experiment. Asking an extra question about time available has hard any physical, psychological, emotional, social and economic effects on the users.\n",
    "\n",
    "**Exposure and Duration**\n",
    "Even for experiments with minimal risk, it is always a good practice to expose the experiment to a smaller subset of traffic whenever possible. This is because no matter how careful you are, there are always some risk involved. For example, there could be a bad implementation problem that causes user unable to complete the enrollment. Running experiment on 100% of the traffic also prevents other experiment being run at the same time.\n",
    "\n",
    "On the other side, the trade-off to exposing only part of the users to experiments is longer experiments\n",
    "duration. Running longer experiments might be more costly (ie. idle resources waiting for experiment result). One might also argue, a shorter duration is preferred in an \"Agile\" environment. As it allows for shorter feedback cycle.\n",
    "\n",
    "The decision on the duration and exposure trade-off is ultimately subjective and depends on external factors other than the experiment itself. Personally, I would divert 50% of the traffic to this experiment. It would take 35 days to complete. If everything go well and there is no other experiment need to run at the same time, I might then speed up the experiment by diverge more traffic to it.\n",
    "\n",
    "## Experiment Analysis\n",
    "### Sanity Checks\n",
    "\n",
    "| Invariant Metrics | Lower bound | Upper bound | Observed | Passes |\n",
    "| --- |--- | --- | --- |\n",
    "| Number of cookies | 0.4988 | 0.5012 | 0.5006 | Yes |\n",
    "| Number of clicks on \"Start free trail\" | 0.4959 | 0.5041 | 0.5005 | Yes |\n",
    "| Click-through-probability on \"Start free trail\" | -0.0013 | 0.0013 | 0.0001 | Yes |\n",
    "\n",
    "The table above shows the lower and upper bound of the confidence interval of each metrics. The observed value for both metrics are within the confidence interval, meaning there is no statically significant difference between the control and the experiment groups for each invariant metrics. Hence,\n",
    "we can continue with our experiment.\n",
    "\n",
    "### Result Analysis\n",
    "\n",
    "| Evaluation Metrics | Lower bound | Upper bound | Statistically significant | Practically significant |\n",
    "| --- | --- | --- | --- |\n",
    "| Gross conversion | -0.029123 | -0.011986 | Yes | Yes |\n",
    "| Net conversion | -0.011605 | 0.001857 | No | No |\n",
    "\n",
    "The table above calculates the lower and upper bounds of the confidence interval of the differences between control and experiment group for evaluation metrics. Because the lower bound for the difference of gross conversion is smaller than zero, we can conclude that the difference of `Gross conversion` is statistically significant. Similarly, because zero is within the CI for `Net conversion`, we conclude that the `Net conversion` is not statistically significant.\n",
    "\n",
    "The practical significant level for `Gross conversion` and `Net conversion` are 0.01 and 0.0075 respectively. Because the absolute value of the lower bound of `Gross conversion` is greater than 0.01, it is practically significant. And because, the practically significant interval of -0.0075 to 0.0075 overlaps with the CI for `Net conversion`, we conclude that the `Net conversion` is *NOT* practically significant.\n",
    "\n",
    "### Sign Tests\n",
    "\n",
    "| Invariant Metrics | p-value | Statistically significant |\n",
    "| --- | --- |\n",
    "| Gross conversion | 0.0026  | Yes |\n",
    "| Net conversion | 0.6776 | No |\n",
    "\n",
    "The p-value for `Gross conversion` is 0.0026, meaning if there is no difference in `Gross conversion`, then the chance of us getting the observed experiment result (signs) is only 0.26%. Because 0.26% is well below our significant level 0f 5%, we conclude the difference for `Gross conversion` is significant.\n",
    "\n",
    "On contrast, the p-value for `Net conversion` is 67%, well above of 5% significant level. Hence it is not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "I didn't use Bonferroni correction.\n",
    "\n",
    "Bonferroni correction is used to correct the false positive probability of a decision rule, when the decision involves multiple hypothesis tests. The reason is that when we have two hypothesis test at 5% significant level. If our decision rule is to launch the experiment when any of the two hypothesis tests return positive result then there is a 9.75% (1-0.95^2) probability of us making a false positive decision, much large than the individual 5%.\n",
    "\n",
    "However, our decision rule is to launch the experiment when there is a significant difference in `Gross conversion` and there is doesn't *not* having a significant difference in `Net conversion`. We actually want to have the second test to be not significant. Not only we want both (vs. any) of the tests to meet our expectation but also the we actually want the second test to be negative. Hence, Bonferroni correction is not applicable here.  \n",
    "\n",
    "There is no discrepancy between the effect size test and sign test. The difference in gross conversion is significant is both tests, and the difference in net conversion is not in both case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation\n",
    "\n",
    "Based on the result, I will recommend launch the experiment.\n",
    "\n",
    "The gross conversion is both statistically and practically significantly lower in the experiment group, This is what we expected in order to launch. Similarly, the net conversion in group experiment is not significantly different from our control. This is also what we expected in order to launch the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-Up Experiment\n",
    "\n",
    "##### Description\n",
    "Competition are frequency used as a motivational tool in education and many other areas (i.e. gaming). It is highly effective when used right. For example, I was hooked on Kaggle Competition since I discovered it throught the machine learning lesson. Udacity could adopt some similar competition mechanism that allows student to compete against each other, something like the best project of the month. The prize of the competition could be something like free tuition for the month. \n",
    "\n",
    "##### Hypothesis\n",
    "The idea is that through healthy competition, student will be more engaged when learning and put more effort toward each project. The improved quality in submission will also reduce the number of average submission for students to pass. This would also free up more Udacity's resource, as less submissions will be required to review.\n",
    "\n",
    "##### Evaluation metrics\n",
    "Here are a couple of metrics I think would be useful:\n",
    "\n",
    "* Average time spend on learning per week\n",
    "* Average number of submission to pass review\n",
    "  \n",
    "##### Invariant metric\n",
    "* Click-throught probability\n",
    "* Number of student enrolled each month\n",
    "\n",
    "##### unit of diversion\n",
    "Given the evaluation metrics, it is nature to user-id as the unit of diversion for this experiment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
